{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca55e43",
   "metadata": {},
   "source": [
    "# Pytorch 基础\n",
    "pytorch 主要有以下几个基础概念：\n",
    "- Tensor：pytorch的核心数据结构，支持多维数组，可以在CPU和GPU上进行加速计算，类似于numpy的ndarray。\n",
    "- Autograd：自动求导机制，支持自动计算梯度，方便进行反向传播和优化。\n",
    "- nn.Module：神经网络模块的基类，用户可以通过继承nn.Module来定义自己的模型。\n",
    "- Optimizer：优化器，用于更新模型参数，支持多种优化算法，如SGD、Adam等。\n",
    "- Device：设备管理，用于指定模型和数据所在的计算设备，如CPU或GPU。\n",
    "\n",
    "## 1. Tensor\n",
    "Tensor是pytorch的核心数据结构，用于存储和操作多维数组。Tensor可以在CPU和GPU上进行加速计算，支持自动求导。\n",
    "Tensor的概念类似于numpy的ndarray，但pytorch的Tensor支持GPU加速计算和自动求导。\n",
    "\n",
    "- Dimensionality: 张量的维度是数据的多维数组结构。例如，标量是0维张量，向量是1维张量，矩阵是2维张量，3D张量是3维张量。\n",
    "- Shape: 张量的形状是一个元组，表示每个维度的大小。例如，一个形状为(3, 4)的张量表示有3行4列。\n",
    "- Data Type: 张量的数据类型可以是整数、浮点数、布尔值等。pytorch支持多种数据类型，如torch.float32、torch.int64等。\n",
    "\n",
    "### 1.1 创建Tensor\n",
    "如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276b956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[ 2.3079,  0.1224, -0.6496],\n",
      "        [ 0.6912, -0.9647, -1.6797]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[ 0.4071,  0.0806, -0.8060],\n",
      "        [ 0.6928, -0.4567, -0.7173]])\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "\n",
    "b = torch.ones(2, 3)\n",
    "print(b)\n",
    "\n",
    "c = torch.randn(2, 3)\n",
    "print(c)\n",
    "\n",
    "import numpy as np\n",
    "numpy_array = np.array([[1, 2], [3, 4]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "print(tensor_from_numpy)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "d = torch.randn(2, 3, device=device)\n",
    "print(d)\n",
    "print(d.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff71f3e5",
   "metadata": {},
   "source": [
    "### 1.2 tensor常用操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6a1d814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6013,  2.2157,  1.1552],\n",
      "        [-0.3049,  1.2532,  1.2434]])\n",
      "tensor([[-0.2025,  1.0725,  0.2101],\n",
      "        [ 0.8307,  1.9045, -0.2094]])\n",
      "tensor([[-0.8038,  3.2883,  1.3653],\n",
      "        [ 0.5258,  3.1577,  1.0340]])\n",
      "tensor([[ 0.1218,  2.3764,  0.2427],\n",
      "        [-0.2533,  2.3867, -0.2603]])\n",
      "tensor([[ 1.6171, -1.5883,  1.1572],\n",
      "        [ 1.5912, -1.1818, -0.6513]])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# plus\n",
    "e = torch.randn(2, 3)\n",
    "print(e)\n",
    "f = torch.randn(2, 3)\n",
    "print(f)\n",
    "print(e + f)\n",
    "\n",
    "# multiply two tensor one-by-one\n",
    "print(e * f)\n",
    "\n",
    "# inverse the tensor\n",
    "g = torch.randn(3, 2)\n",
    "print(g.t()) # or use g.transpose(0, 1)\n",
    "\n",
    "# the shape of tensor\n",
    "print(g.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a574def5",
   "metadata": {},
   "source": [
    "### 1.3 tensor and device\n",
    "Pytorch 张量可以存在于不同的设备上，包括CPU和GPU，可以将张量移动到GPU上加速计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36529730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    tensor_gpu = tensor_from_list.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df0ad4",
   "metadata": {},
   "source": [
    "### 1.4 梯度和自动微分\n",
    "Pytorch张量支持自动微分，这是深度学习中的关健特性，当创建一个需要梯度的张量时，pytorch可以自动计算其梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71984bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# create a required grad's tensor\n",
    "tensor_requires_grad = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# do some operation\n",
    "tensor_result = tensor_requires_grad * 2\n",
    "\n",
    "# cal. the grad\n",
    "tensor_result.backward()\n",
    "print(tensor_requires_grad.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd7905",
   "metadata": {},
   "source": [
    "#### 1.5 memory and performance\n",
    "pytorch 张量提供了一些内存管理功能，如`.clone()`、`.detach()`、`.to()`等，可以帮助优化内存使用和提高性能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33888c21",
   "metadata": {},
   "source": [
    "## 2. Autograd\n",
    "自动求导时深度学习框架中的一个核心特性，它允许计算机自动计算数学函数的导数。\n",
    "在深度学习中，自动求导主要用于两个方面，即：在训练神经网络时计算梯度、进行反向传播算法的实现\n",
    "\n",
    "自动求导基于链式法则（chain rule）：这是一个用于计算复杂函数导数的数学法则，链式法则表明，复合函数的导数是其各个组成部分导数的乘积。在深度学习中，模型通常是由许多层组成的复杂函数，自动求导能够高校的计算这些层的梯度。\n",
    "\n",
    "### 2.1 动态图和静态图\n",
    "- 动态图（dynamic graph）：在动态图中，计算图在运行时动态构建，内次执行操作时，计算图都会更新这使得调试和修改变得更加容易。pytorch使用的是动态图\n",
    "- 静态图（static graph）：在静态图中，计算图在开始执行之间构建完成，并且不会改变。TensorFlow最初使用的是静态图，后来支持动态图。\n",
    "\n",
    "Pytorch提供了自动求导功能，通过`autograd`模块来自动计算梯度。`autograd`模块会根据计算图自动计算梯度，并提供了`backward()`方法来执行反向传播。\n",
    "\n",
    "`torch.tensor`对象有一个`requires_grad`属性，当设置为`True`时，表示需要计算梯度。此时pytorch回自动跟踪所有对他的操作，以便在之后计算梯度。\n",
    "通过调用`backward()`方法，可以计算梯度并存储在`grad`属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4b3a289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9646,  0.8199],\n",
      "        [ 0.3243,  0.4873]], requires_grad=True)\n",
      "tensor(15.4595, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# create a tensor with grad\n",
    "x = torch.randn(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# do some operation\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eed1898",
   "metadata": {},
   "source": [
    "### 2.2 反向传播\n",
    "一旦定义了计算图，可以通过 `.backward()`方法来计算梯度。`backward()`方法会自动计算所有叶子节点的梯度，并将结果存储在每个张量的`grad`属性中。\n",
    "\n",
    "在神经网络训练中，自动求导主要用于实现反向传播算法。方向传播是一种通过计算损失函数关于网络参数的梯度来训练神经网络的方法。在每次迭代中，网络的前向传播会计算输出和损失，然后反向传播会计算关于每个参数的梯度，并使用这些梯度来更新参数。\n",
    "\n",
    "反向传播的步骤如下：\n",
    "1. 前向传播：计算模型的输出和损失函数。\n",
    "2. 反向传播：计算损失函数关于模型参数的梯度。\n",
    "3. 更新参数：使用优化器根据计算出的梯度更新模型参数。\n",
    "4. 重复步骤1-3，直到模型收敛或达到预定的训练轮数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df0a5cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5531, 4.2298],\n",
      "        [3.4864, 3.7310]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b195b095",
   "metadata": {},
   "source": [
    "### 2.3 停止梯度计算\n",
    "如果不希望某些张量的梯度被计算（例如，当不需要反向传播时），可以使用`torch.no_grad()`或设置`requires_grad = False`上下文管理器来停止梯度计算。这样可以节省内存和计算资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48590889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use torch.no_grad() to stop the gradient tracking\n",
    "with torch.no_grad():\n",
    "    y = x + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80793dea",
   "metadata": {},
   "source": [
    "## 3. nn.Module\n",
    "神经网络是一种模仿人脑神经元连接的计算模型，由多层节点（神经元）组成，用于学习数据之间的复杂模式和关系。\n",
    "\n",
    "神经网络通过调整神经元之间的连接权重来优化预测结果，这一过程涉及前向传播、损失计算、反向传播和参数更新等步骤。\n",
    "\n",
    "神经网络的类型包括前馈神经网络（Feedforward Neural Network）、卷积神经网络（Convolutional Neural Network）、循环神经网络（Recurrent Neural Network）和长短期记忆网络（Long Short-Term Memory Network）等。\n",
    "\n",
    "pytorch提供了一个非常方便的结构来构建神经网络模型，即`torch.nn.Module`。`nn.Module`是一个基类，用户可以通过继承它来定义自己的神经网络模型。并实现前向传播方法`forward()`。\n",
    "\n",
    "创建一个简单的神经网络："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea6c697e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# define a simple all-connected neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)       # input layer to hidden layer\n",
    "        self.fc2 = nn.Linear(2, 1)       # hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      # ReLU activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# create the network\n",
    "model = SimpleNN()\n",
    "print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412e38c",
   "metadata": {},
   "source": [
    "### 3.1 训练过程\n",
    "1. 前向传播（Forward Propagation）：在前向传播阶段，输入数据通过网络层传递，每层应用权重和激活函数，最终生成输出。\n",
    "2. 计算损失（Loss Calculation）：根据网络的输出和真实标签，计算损失函数的值。常用的损失函数包括均方误差（MSE）、交叉熵损失等。\n",
    "3. 反向传播（Backward Propagation）：反向传播利用自动求导技术计算损失函数关于网络参数的梯度。通过链式法则，计算每一层的梯度，并将其存储在相应的参数中。\n",
    "4. 更新参数（Parameter Update）：使用优化器（如SGD、Adam等）根据计算出的梯度更新网络参数。优化器会根据学习率和梯度来调整参数，以最小化损失函数。\n",
    "5. 迭代（Iteration）：重复步骤1-4，直到模型收敛或达到预定的训练轮数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff0d6625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[-0.4099]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.1966, grad_fn=<MseLossBackward0>)\n",
      "Loss: 1.1966129541397095\n"
     ]
    }
   ],
   "source": [
    "# forward propagation and loss calculation\n",
    "x = torch.randn(1, 2)  # random input tensor\n",
    "\n",
    "# forward pass\n",
    "output = model(x)\n",
    "print(\"Output:\", output)\n",
    "\n",
    "# define a loss function (such as Mean Squared Error)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# imagine we have a target output 1\n",
    "target = torch.randn(1, 1)  # target tensor\n",
    "\n",
    "# calculate the loss\n",
    "loss = criterion(output, target)\n",
    "print(loss)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99132f1c",
   "metadata": {},
   "source": [
    "**优化器**\n",
    ">优化器在训练过程中更新神经网络的参数，以减小损失函数的值，pytorch提供了多种优化器，常用的有：\n",
    "- **SGD（随机梯度下降）**：最基本的优化算法，通过计算每个参数的梯度来更新参数。\n",
    "- **Adam（自适应矩估计）**：结合了动量和自适应学习率的优化算法，通常收敛速度更快。\n",
    "- RMSprop：自适应学习率优化算法，适用于非平稳目标函数。\n",
    "- Adagrad：自适应学习率优化算法，适用于稀疏数据。\n",
    "- Adadelta：自适应学习率优化算法，改进了Adagrad的缺点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "869b0be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated model parameters after one step of optimization:\n"
     ]
    }
   ],
   "source": [
    "# define an optimizer, use Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# training loop\n",
    "optimizer.zero_grad()  # zero the gradients\n",
    "loss.backward()  # backpropagation\n",
    "optimizer.step()  # update the weights\n",
    "print(\"Updated model parameters after one step of optimization:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ce36e",
   "metadata": {},
   "source": [
    "## 4. 训练模型\n",
    "训练模型是机器学习和深度学习中的核心步骤，旨在通过大量数据学习模型参数，以便模型能够对新的、未见过的数据做出准确的预测。\n",
    "\n",
    "训练模型通常包括以下几个步骤：\n",
    "\n",
    "1. **数据准备**：\n",
    "   1. 收集和处理数据，包括数据清洗、标准化和归一化等\n",
    "   2. 将数据分为训练集、验证集和测试集\n",
    "2. 定义模型：\n",
    "   1. 选择合适的模型架构（如决策树、卷积神经网络、循环神经网络等）\n",
    "   2. 初始化模型参数（权重和偏置）\n",
    "3. 定义损失函数：\n",
    "   1. 选择合适的损失函数（如均方误差、交叉熵等）\n",
    "   2. 定义损失函数的计算方法\n",
    "4. 选择优化器：\n",
    "   1. 选择合适的优化算法（如SGD、Adam等）\n",
    "   2. 定义学习率和其他超参数\n",
    "5. 前向传播：\n",
    "   1. 将输入数据传入模型，计算预测输出\n",
    "6. 计算损失：\n",
    "   1. 根据模型的预测输出和真实标签计算损失函数的值\n",
    "7. 反向传播：\n",
    "   1. 利用自动求导计算损失相对于模型参数的梯度\n",
    "8. 参数更新：\n",
    "   1. 根据计算出的梯度和优化器的规则更新模型参数\n",
    "9. 迭代优化：\n",
    "   1. 重复步骤5-8，直到模型收敛或达到预定的训练轮数\n",
    "10. 评估模型：\n",
    "    1. 在验证集上评估模型性能，调整超参数\n",
    "    2. 在测试集上评估最终模型性能，确保模型没有过拟合\n",
    "11. 模型调优：\n",
    "    1. 根据模型在测试集上的表现进行调参，改变学习率，增加正则化等\n",
    "12. 部署模型：\n",
    "    1. 将训练好的模型部署到生产环境中\n",
    "    2. 监控模型性能，定期更新模型以适应新数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc6bb744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.1054\n",
      "Epoch [20/100], Loss: 1.0754\n",
      "Epoch [30/100], Loss: 1.0566\n",
      "Epoch [40/100], Loss: 1.0405\n",
      "Epoch [50/100], Loss: 1.0234\n",
      "Epoch [60/100], Loss: 1.0132\n",
      "Epoch [70/100], Loss: 1.0090\n",
      "Epoch [80/100], Loss: 1.0079\n",
      "Epoch [90/100], Loss: 1.0075\n",
      "Epoch [100/100], Loss: 1.0075\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. define a simple a neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 2)       # input layer to hidden layer\n",
    "        self.fc2 = nn.Linear(2, 1)       # hidden layer to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))      # ReLU activation function\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 2. create the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# 3. define a loss function and an optimizer\n",
    "criterion = nn.MSELoss()     # set the loss function MSE\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # set the optimizer Adam\n",
    "\n",
    "# 4. create a random input tensor and target tensor\n",
    "x = torch.randn(10, 2)   # random input tensor: 10 samples, each with 2 features\n",
    "Y = torch.randn(10, 1)   # 10 target values\n",
    "\n",
    "# 5. training loop\n",
    "for epoch in range(100):  # number of epochs\n",
    "    optimizer.zero_grad()  # zero the gradients\n",
    "    output = model(x)      # forward pass\n",
    "    loss = criterion(output, Y)  # calculate the loss\n",
    "    loss.backward()        # backpropagation\n",
    "    optimizer.step()       # update the weights\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:  # print every 10 epochs\n",
    "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa1c98",
   "metadata": {},
   "source": [
    "每十轮，程序会输出当前的损失之，帮助我们跟踪模型的训练进度和性能，损失之应该会军舰降低，表示模型在不断学习并优化其参数。\n",
    "\n",
    "训练模型是一个迭代的过程，需要不断地调整和优化，直到达到满意的性能。这个过程涉及到大量的实验和调优，目的是使模型在新的、未见过的数据上具有良好的泛化能力。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ccf8ea",
   "metadata": {},
   "source": [
    "## 5. 设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2998ab25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cpu\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)  # move the model to GPU\n",
    "print(next(model.parameters()).device)  # check the device of model parameters\n",
    "\n",
    "x = x.to(device)  # move the input tensor to GPU\n",
    "print(x.device)\n",
    "Y = Y.to(device)  # move the target tensor to GPU\n",
    "print(Y.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccda92e",
   "metadata": {},
   "source": [
    "在训练过程中，所有张量和模型都应该移到同一个设备上（要么都在CPU上，要么都在GPU上），否则会导致错误。可以使用`torch.device`来指定设备，并使用`.to(device)`方法将张量或模型移动到指定设备上。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
