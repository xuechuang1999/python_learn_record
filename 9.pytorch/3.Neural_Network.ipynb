{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f085e2b8",
   "metadata": {},
   "source": [
    "# PyTorch神经网络基础\n",
    "神经网络是一种模仿人脑处理信息方式的计算模型，它由许多互相连接的节点（或称为神经元）组成。这些节点按层次排列，每个节点接收输入信号，进行处理后输出结果。神经网络可以用于分类、回归等多种任务。\n",
    "\n",
    "神经网络的特变指出在于其能够自动从大量数据中学习复杂的模式和特征，无需人工设计特征提取器。随着深度学习的发展，神经网络已经成为解决许多复杂问题的关键因素。\n",
    "\n",
    "## 1. 神经元（Neuron）\n",
    "神经元是神经网络的基本单元，它接收输入信号，通过加权求和后与偏置（bias）相加，再通过激活函数（activation function）进行非线性变换，最后输出结果。\n",
    "\n",
    "神经元的权重和偏置是网络学习过程中需要调整的参数。通过反向传播算法（backpropagation），神经网络可以根据输出结果与真实标签之间的误差，自动调整这些参数，以最小化误差。\n",
    "\n",
    "### 输入与输出：\n",
    "- 输入（Input）：神经元接收的信号，可以是来自其他神经元的输出，也可以是原始数据--特征数据，如图像的像素值或文本的词向量；\n",
    "- 输出（Output）：网络的重点，表示模型的预测结果，如分类任务中的类别标签。\n",
    "\n",
    "神经元接收多个输入（例如 $x_1, x_2, \\ldots, x_n$），如果输入的加权和大于激活阈值（activation potential），则神经元被激活，输出1；否则输出0，即产生二进制输出。激活函数通常是非线性的，如Sigmoid、ReLU等。\n",
    "\n",
    "![alt text](<assets/Neuron network.png>)\n",
    "\n",
    "神经元的输出可以看作是输入的加权和加上偏置(Bias)，神经元的数学表示为：\n",
    "$$\n",
    "output = \\sum_{i=1}^{n} w_i \\cdot x_i + Bias\n",
    "$$\n",
    "其中，$w_i$ 是输入 $x_i$ 的权重，$Bias$ 是偏置项。\n",
    "### 激活函数（Activation Function）\n",
    "激活函数是神经元输出的非线性变换函数，它决定了神经元的输出值。常用的激活函数有：\n",
    "- Sigmoid：将输入值映射到0到1之间，适用于二分类问题；\n",
    "- ReLU（Rectified Linear Unit）：将负值映射为0，正值不变，适用于深度神经网络；\n",
    "- Tanh：将输入值映射到-1到1之间，适用于二分类问题；\n",
    "- Softmax：将输入值转换为概率分布，适用于多分类问题。\n",
    "- Leaky ReLU：在ReLU的基础上，允许负值有小的斜率，避免神经元死亡问题。\n",
    "- ELU（Exponential Linear Unit）：在ReLU的基础上，增加了负值的指数衰减，避免神经元死亡问题。\n",
    "- Swish：结合了Sigmoid和ReLU的优点，具有平滑的非线性特性。\n",
    "- GELU（Gaussian Error Linear Unit）：结合了高斯分布和线性函数，具有平滑的非线性特性。\n",
    "- Mish：结合了Swish和Tanh的优点，具有平滑的非线性特性。\n",
    "- Softplus：平滑的ReLU函数，避免了ReLU的非光滑性。\n",
    "- Softsign：平滑的Tanh函数，避免了Tanh的非光滑性。\n",
    "- Hard Sigmoid：近似Sigmoid函数，计算速度更快。\n",
    "- Hard Swish：近似Swish函数，计算速度更快。\n",
    "- Hard Tanh：近似Tanh函数，计算速度更快。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143a201",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 2. 层（Layer）\n",
    "输入层与输出层之间的层被称为隐藏层，层与层之间的连接密度和类型构成了网络的配置。\n",
    "\n",
    "神经网络由多个层组成，包括：\n",
    "- 输入层（Input Layer）：接收原始输入数据；\n",
    "- 隐藏层（Hidden Layer）：对输入数据进行特征提取和转换，通常有多个；\n",
    "- 输出层（Output Layer）：输出最终的预测结果。\n",
    "\n",
    "典型的神经网络架构如下：\n",
    "\n",
    "![alt text](<assets/Classical_NN_structure.webp>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b324a",
   "metadata": {},
   "source": [
    "### 3. 前馈神经网络（Feedforward Neural Network, FNN）\n",
    "前馈神经网络是神经网络家族中的基本单元。\n",
    "\n",
    "前馈神经网络特点是数据从输入层开始，经过一个或多个隐藏层，最终达到输出层，全过程没有循环或反馈连接。每一层的输出作为下一层的输入，数据在网络中单向流动。\n",
    "\n",
    "![alt text](<assets/FNN.png>)\n",
    "\n",
    "**前馈神经网络的基本结构包括**：\n",
    "- 输入层：数据进入网络的入口点，输入层的每个节点代表一个输入特征；\n",
    "- 隐藏层：一个或多个层，用于捕获数据的非线性特征，每个隐藏层由多个神经元组成，每个神经元通过激活函数增加非线性能力；\n",
    "- 输出层：输出网络的预测结果。节点数和问题类型相关，例如分类问题的输出节点数等于类别数；\n",
    "- 连接权重与偏置：每个神经元的输入通过权重进行加权求和，并加上偏置值，然后通过激活函数传递。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec8fa6",
   "metadata": {},
   "source": [
    "## 4. 循环神经网络结构（Recurrent Neural Network, RNN）\n",
    "循环神经网络（RNN）是一类专门处理序列数据的神经网络，能够捕获输入数据中时间或顺序信息的依赖关系。RNN的基本思想是将前一时刻的输出作为当前时刻的输入之一，从而形成一个循环结构。\n",
    "\n",
    "RNN的特别之处在于它具有记忆能力，可以在网络的隐藏状态中保存之前时间步的信息。循环神经网络用于处理时间变化的数据模式。\n",
    "\n",
    "在RNN中，相同的层被用来接收输入参数，并在指定的神经网络中显示输出参数。\n",
    "\n",
    "![alt text](<assets/RNN.png>)\n",
    "\n",
    "\n",
    "## 5. Pytorch中的神经网络\n",
    "PyTorch提供了相关的工具来构建和训练神经网络。神经网络在Pytorch中是通过`torch.nn`模块来实现的。我们可以使用`torch.nn.Module`类来定义一个神经网络模型，并在其中定义各个层和前向传播的方法。\n",
    "\n",
    "`torch.nn`模块提供了各种网络层（如全连接层、卷积层、池化层等）、损失函数和优化器等，可以方便地构建和训练复杂的神经网络模型。\n",
    "\n",
    "![alt text](<assets/pytorch_nn.png>)\n",
    "\n",
    "在pytorch中，构建神经网络通常需要继承 `torch.nn.Module` 类。\n",
    "\n",
    "`nn.Module`是所有神经网络模块的基类，通常需要定义以下两个部分：\n",
    "- `__init__`：定义网络的结构，包括各个层的初始化；\n",
    "- `forward`：定义前向传播的计算过程，指定输入数据如何通过网络进行处理。\n",
    "\n",
    "简单的全连接神经网络（FNN）示例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d99ad53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
      "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# define a simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # define a fully connected layer from input to hidden layer\n",
    "        self.fc1 = nn.Linear(2, 2)           # input size 2, output size 2\n",
    "        # define a fully connected layer from hidden to output layer\n",
    "        self.fc2 = nn.Linear(2, 1)           # input size 2, output size 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward propagation process\n",
    "        x = torch.relu(self.fc1(x))          # apply ReLU activation function\n",
    "        x = self.fc2(x)                      # pass through the second layer\n",
    "        return x                            # return the output\n",
    "\n",
    "# create an instance of the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# print the model architecture\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4a18a",
   "metadata": {},
   "source": [
    "PyTorch 提供了许多常见的神经网络层，以下是几个常见的：\n",
    "\n",
    "- `torch.nn.Linear(in_features, out_features)`：全连接层，输入特征数为 `in_features`，输出特征数为 `out_features`；\n",
    "- `torch.nn.Conv2d(in_channels, out_channels, kernel_size)`：二维卷积层，输入通道数为 `in_channels`，输出通道数为 `out_channels`，卷积核大小为 `kernel_size`， 用于图像处理；\n",
    "- `torch.nn.MaxPool2d(kernel_size)`：二维最大池化层，池化核大小为 `kernel_size`，用于下采样，即降维；\n",
    "- `torch.nn.ReLU()`：ReLU激活函数层，常用于隐藏层；\n",
    "- `torch.nn.Softmax(dim)`：Softmax激活函数层，通常用于输出层，适用于多分类问题，`dim`指定在哪个维度上进行归一化；\n",
    "\n",
    "\n",
    "- `torch.nn.CrossEntropyLoss()`：交叉熵损失函数，通常用于分类任务；\n",
    "- `torch.optim.SGD(params, lr)`：随机梯度下降优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.Adam(params, lr)`：Adam优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.AdamW(params, lr)`：AdamW优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.RMSprop(params, lr)`：RMSprop优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.Adagrad(params, lr)`：Adagrad优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.Adamax(params, lr)`：Adamax优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.NAdam(params, lr)`：NAdam优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.LBFGS(params, lr)`：LBFGS优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.SparseAdam(params, lr)`：SparseAdam优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.SparseAdam(params, lr)`：SparseAdam优化器，`params`是需要优化的参数，`lr`是学习率；\n",
    "- `torch.optim.NAdam(params, lr)`：NAdam优化器，`params`是需要优化的参数，`lr`是学习率；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f15ba",
   "metadata": {},
   "source": [
    "## 6. 损失函数（Loss Function）\n",
    "损失函数是用来评估模型预测结果与真实标签之间差距的函数。它是神经网络训练过程中需要最小化的目标函数。常用的损失函数有：\n",
    "- 均方误差（Mean Squared Error, MSELoss）：用于回归任务，计算预测值与真实值之间的平方差；\n",
    "- 交叉熵损失（CrossEntropyLoss）：用于分类任务，计算预测概率分布与真实标签之间的差异；\n",
    "- 二元交叉熵损失（BCEWithLogitsLoss）：用于二分类任务，计算预测值与真实标签之间的差异，结合了Sigmoid激活函数和BCELoss；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4aa67a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function: MSELoss()\n",
      "Cross-Entropy Loss function: CrossEntropyLoss()\n",
      "BCEWithLogits Loss function: BCEWithLogitsLoss()\n"
     ]
    }
   ],
   "source": [
    "# Mean Squared Error (MSE) loss function\n",
    "criterion = nn.MSELoss()\n",
    "print(\"Loss function:\", criterion)\n",
    "\n",
    "# Cross-Entropy loss function\n",
    "criterion_ce = nn.CrossEntropyLoss()\n",
    "print(\"Cross-Entropy Loss function:\", criterion_ce)\n",
    "\n",
    "# BCEWithLogits loss function\n",
    "criterion_bce = nn.BCEWithLogitsLoss()\n",
    "print(\"BCEWithLogits Loss function:\", criterion_bce)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74eb4ba",
   "metadata": {},
   "source": [
    "## 7. 优化器（Optimizer）\n",
    "优化器负责在训练过程中更新网络的权重和偏置。\n",
    "\n",
    "常见的优化器包括：\n",
    "- 随机梯度下降（SGD）：最基本的优化器，通过计算损失函数的梯度来更新参数；\n",
    "- Adam：自适应学习率优化器，结合了动量和RMSprop的优点，适用于大规模数据和高维参数空间；\n",
    "- Adagrad：自适应学习率优化器，适用于稀疏数据；\n",
    "- RMSprop：自适应学习率优化器，适用于非平稳目标；\n",
    "- Adadelta：自适应学习率优化器，改进了Adagrad的缺点；\n",
    "- AdamW：Adam优化器的改进版本，增加了权重衰减（weight decay）；\n",
    "- Nadam：结合了Adam和Nesterov动量的优化器；\n",
    "- LBFGS：拟牛顿优化器，适用于小规模数据集；\n",
    "- Rprop：适用于小规模数据集的优化器；\n",
    "- FTRL：适用于大规模数据集的优化器；\n",
    "- RMSprop：适用于非平稳目标的优化器；\n",
    "- Rprop：适用于小规模数据集的优化器；\n",
    "- FTRL：适用于大规模数据集的优化器；\n",
    "- LARS：适用于大规模数据集的优化器；\n",
    "- LAMB：适用于大规模数据集的优化器；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56669e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer: SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "Adam Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    decoupled_weight_decay: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# use Stochastic Gradient Descent (SGD) optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # learning rate 0.01\n",
    "print(\"Optimizer:\", optimizer)\n",
    "\n",
    "# use Adam optimizer\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)  # learning rate 0.001\n",
    "print(\"Adam Optimizer:\", optimizer_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f704b5a5",
   "metadata": {},
   "source": [
    "## 8. 训练过程（Training Process）\n",
    "训练神经网络的过程通常包括以下几个步骤：\n",
    "1. **数据准备**：加载和预处理数据集，将数据划分为训练集、验证集和测试集，通过`torch.utils.data.DataLoader`来加载数据；\n",
    "2. **模型定义**：定义神经网络模型，通常需要继承`torch.nn.Module`类，并实现`__init__`和`forward`方法；\n",
    "3. **损失函数和优化器定义**：选择合适的损失函数和优化器；\n",
    "4. **前向传播**：将输入数据传入模型，计算输出结果；\n",
    "5. **计算损失**：使用损失函数计算模型输出与真实标签之间的差距，得到损失值；\n",
    "6. **反向传播**：通过`loss.backward()`计算梯度；\n",
    "7. **参数更新**：使用优化器更新模型参数，通过`optimizer.step()`实现；\n",
    "8. 重复上述步骤，直到达到预设的训练轮数或损失收敛；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fc0ca64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 1.5729\n",
      "Epoch [20/100], Loss: 1.5428\n",
      "Epoch [30/100], Loss: 1.5194\n",
      "Epoch [40/100], Loss: 1.5009\n",
      "Epoch [50/100], Loss: 1.4859\n",
      "Epoch [60/100], Loss: 1.4737\n",
      "Epoch [70/100], Loss: 1.4636\n",
      "Epoch [80/100], Loss: 1.4551\n",
      "Epoch [90/100], Loss: 1.4479\n",
      "Epoch [100/100], Loss: 1.4417\n"
     ]
    }
   ],
   "source": [
    "# assume we have defined the model, loss function, and optimizer\n",
    "\n",
    "# an instance of training data\n",
    "X = torch.randn(10, 2)  # 10 samples, and each sample has 2 features\n",
    "y = torch.randn(10, 1)  # 10 target labels, each with 1 value\n",
    "\n",
    "# training loop\n",
    "for epoch in range(100):     # train for 100 epochs\n",
    "    model.train()          # set the model to training mode\n",
    "    optimizer.zero_grad()  # clear the gradients\n",
    "    outputs = model(X)     # forward pass\n",
    "    loss = criterion(outputs, y)  # compute the loss\n",
    "    loss.backward()        # backpropagation\n",
    "    optimizer.step()       # update the weights\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:  # print every 10 epochs\n",
    "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')\n",
    "        # print the loss value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195391cc",
   "metadata": {},
   "source": [
    "## 9. 模型评估（Model Evaluation）\n",
    "训练完成后，需要对模型进行评估，以了解其在测试集上的性能。常用的评估指标包括：\n",
    "- 准确率（Accuracy）：分类任务中正确预测的样本数与总样本数之比；\n",
    "- 精确率（Precision）：分类任务中正确预测的正类样本数与预测为正类的样本数之比；\n",
    "- 召回率（Recall）：分类任务中正确预测的正类样本数与实际正类样本数之比；\n",
    "\n",
    "步骤为：\n",
    "- **计算测试集的损失**：测试模型在未见过的数据上的表现；\n",
    "- **计算准确率**：对于跟类问题，计算正确预测的比例；\n",
    "- **绘制混淆矩阵**：可视化分类结果，帮助分析模型的分类性能；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a9b10af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4411\n"
     ]
    }
   ],
   "source": [
    "# assume we have test data: X_test, Y_test\n",
    "model.eval()  # set the model to evaluation mode\n",
    "with torch.no_grad():  # no need to compute gradients\n",
    "    test_outputs = model(X)  # forward pass on test data\n",
    "    test_loss = criterion(test_outputs, y)  # compute the loss\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')  # print the test loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4b5daa",
   "metadata": {},
   "source": [
    "## 10. 神经网络类型\n",
    "1. 前馈神经网络（Feedforward Neural Network, FNN）：最基本的神经网络类型，数据从输入层经过隐藏层到达输出层，没有循环或反馈连接；\n",
    "2. 卷积神经网络（Convolutional Neural Network, CNN）：主要用于图像处理，通过卷积层提取特征，适用于图像分类、目标检测等任务；\n",
    "3. 循环神经网络（Recurrent Neural Network, RNN）：主要用于处理序列数据，具有记忆能力，适用于自然语言处理、时间序列预测等任务；\n",
    "4. 长短时记忆网络（Long Short-Term Memory, LSTM）：RNN的一种变体，解决了RNN在长序列数据中梯度消失和爆炸的问题，能够学习长期依赖关系。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
