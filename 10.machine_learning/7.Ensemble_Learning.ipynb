{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ba92a03",
   "metadata": {},
   "source": [
    "# 集成学习（Ensemble Learning）\n",
    "集成学习是一种通过结合多个模型的预测结果来提高整体性能的技术。\n",
    "\n",
    "通过多个弱学习器的组合，构建一个强学习器，从而提升模型的准确性和鲁棒性。常见的集成学习方法包括Bagging、Boosting和Stacking等。\n",
    "\n",
    "常见的集成学习方法有：\n",
    "1. Bagging（Bootstrap Aggregating）：通过自助采样法（Bootstrap sampling）生成多个训练集，训练多个独立的模型，然后将它们的预测结果进行投票或平均得到最终结果。\n",
    "2. Boosting：通过迭代的方式训练多个模型，每个模型都试图纠正前一个模型的错误，最终将这些弱学习器的预测结果进行加权组合得到结果。\n",
    "3. Stacking：通过训练多个不同类型的模型，然后使用一个元模型（meta-model）来学习如何最佳地组合这些模型的预测结果。\n",
    "\n",
    "## 1. Bagging（Bootstrap Aggregating）\n",
    "Bagging 的目标是通过减少模型的方差来提高性能，适用于高方差、易过拟合的模型。通过以下步骤实现：\n",
    "- 数据集重采样：对原始训练数据集进行多次有放回的随机采样，生成多个不同的子训练集。\n",
    "- 训练多个模型：在每个子数据集上训练一个基学习器（通常是相同类型的模型）。\n",
    "- 结果合并：将多个基学习器的预测结果进行投票（分类任务）或平均（回归任务）得到最终预测结果。\n",
    "\n",
    "### 典型算法：\n",
    "- 随机森林（Random Forest）：通过Bagging方法构建多个决策树，并在每个节点上随机选择特征进行分裂，从而增加模型的多样性。降低过拟合风险，提高预测性能。\n",
    "\n",
    "<font color=blue>优势</font>：\n",
    "- 可以有效减小方差，提高模型的稳定性和准确性。\n",
    "- 适用于高方差、易过拟合的模型。\n",
    "\n",
    "<font color=red>劣势</font>：\n",
    "- 训练过程较为耗时，因为需要训练多个模型。\n",
    "- 结果难以解释，因为是多个模型的组合，无单一的模型。\n",
    "\n",
    "## 2. Boosting\n",
    "Boosting 的目标是通过减少模型的偏差来提高性能，适用于弱学习器。Boosting 的核心思想是逐步调整每个模型的权重，强调那些被前一个模型错误分类的样本。通过以下步骤实现：\n",
    "- 序列化训练：依次训练多个基学习器，每个学习器都关注前一个学习器分类错误的样本。\n",
    "- 加权投票：将多个基学习器的预测结果进行加权组合，得到最终预测结果。其中错误分类的样本会被赋予更高的权重。\n",
    "- 合并结果：将多个基学习器的预测结果进行加权组合得到最终预测结果，权重根据在训练过程中的表现来确定。\n",
    "### 典型算法：\n",
    "- AdaBoost（Adaptive Boosting）：通过调整样本权重来训练多个弱学习器，使得每个后续分类器更加关注前一个分类器错误分类的样本。\n",
    "- 梯度提升树（Gradient Boosting Tree）：通过迭代优化目标函数，逐步减小方差。\n",
    "- XGBoost（Extreme Gradient Boosting）：一种高效的梯度提升实现，具有正则化功能，防止过拟合。广泛应用于各种机器学习竞赛中，具有较强的性能和速度优势。\n",
    "- LightGBM：一种基于梯度提升树的框架，采用基于直方图的决策树学习算法，相较于XGBoost具有更快的训练速度和更低的内存消耗，适用于大规模数据集。\n",
    "\n",
    "<font color=blue>优势</font>：\n",
    "- 适用于偏差较大的模型，能有效提高模型的准确性。\n",
    "- 性能强大，在许多实际应用中表现出色。\n",
    "\n",
    "<font color=red>劣势</font>：\n",
    "- 对噪声数据敏感，容易过拟合。\n",
    "- 训练过程较慢，特别是在数据量较大的情况下。\n",
    "\n",
    "## 3. Stacking （stacked generalization）\n",
    "stacking 通过结合多个不同类型的模型来提高预测性能。\n",
    "并使用一个元模型（meta-model）来学习如何最佳地组合这些模型的预测结果。通过以下步骤实现：\n",
    "- 第一层（基学习器）：训练多个不同类型的基学习器（如决策树、支持向量机、神经网络等）来对数据进行预测；\n",
    "- 第二层（元学习器）：使用第一层基学习器的预测结果作为输入，训练一个元学习器（通常是线性回归、逻辑回归或神经网络等）来进行最终的预测。\n",
    "\n",
    "### 典型算法：\n",
    "- 多层次堆叠（Multi-level Stacking）：通过多层次的堆叠结构，进一步提升模型的性能。\n",
    "- 混合模型（Hybrid Models）：结合不同类型的基学习器和元学习器，充分利用各自的优势。\n",
    "- 神经网络堆叠（Neural Network Stacking）：使用神经网络作为元学习器，能够捕捉复杂的非线性关系。\n",
    "- 贝叶斯堆叠（Bayesian Stacking）：通过贝叶斯方法来优化基学习器的权重分配，提高模型的泛化能力。\n",
    "\n",
    "<font color=blue>优势</font>：\n",
    "- 能够结合不同类型的基学习器，捕捉数据中不同的模式；\n",
    "- 理论上可以结合多种模型的优势，提升整体性能，达到更好的预测效果。\n",
    "\n",
    "<font color=red>劣势</font>：\n",
    "- 训练过程复杂，需要调试多个模型和元学习器，且模型间的结合方式需要精心设计；\n",
    "- 相较于其它集成方法，如Bagging和Boosting，更复杂，且容易过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d4d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# train model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e182f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\23210\\.conda\\envs\\sklearn\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Boosting: AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# use the defaultaly decision tree as the base estimator\n",
    "# and specify use SAMME algorithm\n",
    "ada_model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                               n_estimators=50, \n",
    "                               algorithm=\"SAMME\", \n",
    "                               random_state=42)\n",
    "\n",
    "# train model\n",
    "ada_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = ada_model.predict(X_test)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fdbd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# stacking\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# define base learners\n",
    "base_learners = [\n",
    "    ('dt', DecisionTreeClassifier(max_depth=3)),\n",
    "    ('svc', SVC(probability=True))\n",
    "]\n",
    "\n",
    "# create stacking classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=LogisticRegression(),\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# train model\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = stacking_model.predict(X_test)\n",
    "\n",
    "# evaluate model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
